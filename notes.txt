do not take the diag but average a few points around it.
pooler les essais du loc et de 1obj pour améliorer la generalization.?

BACK TO V1
c1: windows: .15-.3s for early and .4-.55 for late
v2: windows now .1-.4s for early and .45-.75s for late.
v3: cat10, no reduc dim. All launching all decoding. 
v4: now clipping - baselining - clipping again. All conds, including delay windowing 
v5: reduc dim for single ch decoding + smaller time window
v6: including the presence of a flash in the metadata. Also Decoding cimplexitiy with generalization (sameShape -> sameColor). + mean 3
v7: testing split queries for OVR + better way to arange for split queries aleas. TO BE REPLICATED IN THE NORMAL DECODING
v8: quality threshold = 0.75 (decoding matching in a signle block with perf >.75) -> worse than original.
v9: new Epochs named Epochs_filt with lowpass=30hz, highpass=0.1hz (instead of 0 and 0.01), - Launched both at 13h55 
Finally corrected split_queries for OVR ! to be tested
Corrected test-cond as well, there was a bug due to the change of shape of all_models (not for OVR)
Actually the new epochs is the same as the old, more or less. The sfreq is 50hz, so there is a 25hz lowpass. Only highpass at 0.1hz is different
v10: autoreject + filter with Perf==1 for all - good ! but still "hashy", need to smooth more. Maybe increase sfreq too...
v11: Simple ridgeCV and equalize events for all conds (check for OVR if it works!) + trying complexity regression decoding (with equalize too, maybe try without) + single_channel decoding.
add complexity decoding!
v12: 100hz - still with ridgeCV ! - still equalize_events
v13: 5cat and 11smooth (insread of 21) regression only - CHANGING, now 10cat 21smooth
v14: 10cat regression only - still equalize_events
v15: no cat no mean no equalize events -> best !
v16: 21smooth, Ridge, no equalize. SameX OVR, mismatch with complexity splits, normal and OVR
v17: response-lock normal decoding + decoding window with XGboost, no reduc dim, no param search
v18: all LRcv with 5cat - 21smooth + also windowXGBoost same as v17, corrected some queries (but not all it seems) -- small problem on resplock: cropped to epochs before getting new epochs -- pretty good perf !
decoding window with XGboost, .99reduc dim, no param search, micro_ave=2, max_trials_win=5000, sfreq_win=50hz -> bof
v19: 50hz ; trial micro-averaging, 2000 trials. Trial microaveraging makes split queries VERY costly. takes 5x time to run. So removed most split queries for v19.
+ decoding window with XGboost, .99reduc dim, no param search, micro_ave=2, max_trials_win=False, sfreq_win=50hz. new: left color and shape, mismatch side. New windows: from 100 to 700ms after word onset. 600ms during the delay.
v20: Pyriemann? Works only for windows decoding, because the covariance is computed over time (the time dimension is lost). Perf is very bad
Trying again without pyriemann but with equalize events. 20000max trials, 50hz, .99reducdim
also trying side mismatch decoding with micro_ave=3, equalize events, 10000maxtrials, window5-8.5s.
Also trying CCA
launched v21, all freq bands -- watch out low and high gamma already failed due to hfreq > sfreq
v22: filtering 0.01hz-100hz - not much better? maybe a bit better!
v23: proper CCA with cat10, nofitler, 50hz. 
dimensionality, cat10, with trials averaging, but its shit so trying without trials averaging
Maybe try subtract evoked?
v24: master decoder: 100hz, nofilter, cat5, autoreject, NO micro-average. for color, shape and mismatch only.
Not excellent. 
Launched regression and Side with Not the same color on the other side.
v25, 26, 27, looking for best perf. 25 is bad. 26 takes forever.

Dimensionality: NOW using time windows !
v28 = copy of v25 but with linear detrending
v29: same with localizer (single words loc). Also Decoding ovr with auc_thresh=.55
v31: dimensionality analysis with singleobjects localizer -> bof, trying with averaging trials
v32: dim with cross word-img localzier, bof trying with avertaging trials. bof, trying without detrending.
v33: dim with riemann. bof, trying overlapping windows


Gen of localised decoders : do we see the maintenance of representation waiting to be composed? Probably not. + for classifiers trained on one obj blocks, the one should be only to the following property!
-> well first the shape decoder doesnt generalize well from loc to blocks ...
Then, how do you get the "diagonal" generalization performance? To be fair you would need to select a single decoder and test it on each timepoint.

Dimensionality analysis: decrease during the delay? Bigger for less complex sentences? Bingo ! Not really the case ...
remove windows? do not need them since we use the localizer.

Localizer for decoding. -> ok for lots of conditions ! should try with more folds

Test new localiser with both words and images. Should be done by Monday evening. 




cosine distance between decoders, as a timegen?


TODO: test on subset of all analyses! eg: prop of one object, button, complexité.
Temps-fréquence?

Maybe use only half of the trials (even-odd) LAUNCHED for v18 and 19 (19 looks best) -> perf is too low because we use only half of the "positive"class trials. 
-> doing the split only for the "negative" class. Launched for v18 and v19.

RT split (3 quantiles)



XGBoost with all subjects? -> chance level even for easy stuff...
SameX has super generalization because the negative class is the same in train and test generalization
remove perf filtering for windows analyses?
XGBoost on all subjects at once? With regression for each property. But need an overall PCA, not in the pipeline obj.

Resplock for regression? and for window?


smooth AUC curve ?

##, reducdim=0.99, 5folds, parameters search with 200iter


no Cmismatch and Smismatch yet + oneobj Perf, Matching and Button, need to relaunch OVR decoding
Generalization of decoders? Using sliding windows
Check the windows of single channels decoding: should it be 1 SOA ?
use only windows for testing generalization of proprty decoders?  with offset for subject with slower presentation



Milad Mozafari: decoding text from brain representations of images works very well (nearly as good as predicting text from text), but the other way around is very bad ...



jobs: 


change Error_type to lower case? Would be great for grepping logs.

np.diagonal(array, offset) to get off-diagonals

RSA on image presentation to see if the similarity is bigger when the shape matches, or the color




Use principal angles between subspaces? https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.subspace_angles.html
angle between first and second color (or shape) encoding in scenes ? How is it compared to signle objects?

make ICA separately for each block!!! or not at all...

Binding condition: triangle bleue vs other triangles, generalized to triangle bleue vs other triangles when the other object is blue (and optionally also not a triangle)



Preprocessing notes:
bad subjects (chance-level AUC) : 03 - 09 - 10 - 12 - 14
weird perf before t0: 07 - 11 - 13 - 16

to look into: 
sub01 - very weird looking PSD on 1, 2, 4 and 5, - run4: shity spiked dominating the evoked.
sub02 - run3 (one_obj). Weird very high amplitude ramping.
sub03 - run8 (1obj) : that asshole kept the button pressed for a whole trial. Maybe trials are shifted, resulting in bad decoding perf.
sub06: run1 one response was too quick. Wwird peak in the PSD at ! 12hz and ~30hz. Linked to construction work ? Run5-7-8-9: sustained diverging channels.
sub07 - run5 2obj: delay in one trial, maybe trials are shifted. Also same peak in PSD.
sub08 - run3 2obj: answered once during the presentation of the sentence. same peak in PSD.
sub09: same peak in PSD but even bigger. Huge ammount of noise in frontal sensors. Also looks like there is a jitter on the image presentation, also for sub08 but not really before (maybe on the onset but not the end).
sub10 - same jitter, same peak in PSD, also hige ammount of noise all over. 
sub11 - run1: one press during trial + run4 on during delay. run5: huge jumps + one huge fuckup in the triggers. Trials may be shifted. One button press at the begining of a trial in run6. Super fucking weird channel jumps in run7. On press before image on run 8. Super weird high sustained channels on run 9 + one press during the image. What a clusterfuck!
sub12 - run4 and 7: fckin weird ch jump. run5-6-7: weird peak in the PSD at 55hz... no run 10 ?
sub13 - run1: multiple answers during trials. Weird ch jumps in run7. No run10.
sub14: just fucking noisy all over.
sub15 - run1: button press jsut before a trial. run6: one trial is shifted. 
sub16: bit noisy but looks ok. problem in run8?



Meeting 27-10-20:
Mesure résumée : pool par fenetre après chaque mot et moyenne
La généralisation est-elle "diagonale" après le mot, ou bien off-diagonale ? Si on a un processing très différent on peut s'attendre a du off-diagonale. 
Decoder la position des mots ? Ou bien l'inclure dans la RSA. 
Inclure un flash sur tout l'écran, un truc gris ou salt and pepper, sur la moitié des essais, 500ms avant l'image. -> flash = trigger 5 
Figure finale : on veut un truc a la JR ou a la Pedro avec un timecourse par propriété d'interet: forme1, couleur1, relation, forme2, couleur2, objet individuels, match/nonmatch 
Trace de la prédiction pour chaque décodeur : quand est-ce que la perf est bonne ET quand est-ce qu'on a une réactivation. On peut les moyenner comme on le fait pour l'AUC. 
Quand on a une réactivation, est ce qu'elle est jointe pour nom et adj (forme et couleur), ou bien séparée ? Quel délai ? Comment sont ségrégées les réactivations d'un objet et de l'autre ? => binding problem.




Reprise en 2024 ! 
v1: replication (more or less, maybe check v34 for a closer replication)
v2: trying localizer crossmodal generalisation
v3: saving confusion matrices for RSA (9words, shapes and colors?) and full predictions for replays stuuuff.
Saving preds works. we save only the diagonal (within-time) predictions, but for the generalization across conditions of a different length (eg, loc to obj), then we have to save the whole matrix, which is fckin big. Maybe soon look into single clf gen? 

v4: trying single time point training and gen to specific window. 
v5: same with no timegen (no need for replays)

DONE 
single decoder (no kfold)
find why the plotting script does not find all the timepoints
save patterns with:
        # Computes patterns using Haufe's trick: A = Cov_X . W . Precision_Y
        inv_Y = 1.0
        X = X - X.mean(0, keepdims=True)
        if y.ndim == 2 and y.shape[1] != 1:
            y = y - y.mean(0, keepdims=True)
            inv_Y = np.linalg.pinv(np.cov(y.T))
        self.patterns_ = np.cov(X.T).dot(self.filters_.T.dot(inv_Y)).T

TODO:
Do the null trial thing. 
find optimal C (or do a crossval?)

v6: problems solved, LRCV, just no null trials. Saving patterns and preds correctly 